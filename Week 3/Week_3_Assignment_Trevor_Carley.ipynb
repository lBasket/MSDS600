{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "Use the \"from the expert\" (FTE) jupyter notebook as a starter for this assignment, and ask your instructor questions if you need help.\n",
    "\n",
    "Use our saved churn data from week 2 with machine learning to predict if customers will churn or not, similar to what we did in the FTE:\n",
    "\n",
    "- break up data into features and targets\n",
    "- split data into train and test sets\n",
    "- use at least one ML model to fit to the training data\n",
    "- evaluate performance on the train and test sets: at least evaluate accuracy and compare it with the \"no information rate\"\n",
    "- plot a confusion matrix\n",
    "- write something describing how the ML algorithm could be used in a business setting\n",
    "- Write a short summary of what you did with the overall process - describe any important EDA findings, data cleaning and preparation, modeling, and evaluation in your summary.\n",
    "\n",
    "*Optional*: For an addition challenge, try the following:\n",
    "- fit more ML models and compare their scores\n",
    "- optimize the hyperparameters of your models\n",
    "- examine more metrics such as the classification report and ROC/AUC\n",
    "- plot the distribution of the probability predictions (from the `predict_proba()` function from our model) for each class (1s and 0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS process status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our data science process, and where we are (#4):\n",
    "\n",
    "**1. Business understanding**\n",
    "\n",
    "Can we use machine learning to predict if a customer will churn before they leave?\n",
    "\n",
    "**2. Data understanding**\n",
    "\n",
    "Week 1 - EDA and visualization.\n",
    "\n",
    "**3. Data preparation**\n",
    "\n",
    "Last week - cleaning and feature engineering.\n",
    "\n",
    "\n",
    "**4. Modeling**\n",
    "\n",
    "This week.\n",
    "Fit a ML model to the data.\n",
    "\n",
    "\n",
    "**5. Evaluation**\n",
    "\n",
    "This week.\n",
    "Check the performance of our models and evaluate how it fits our goals from step 1.\n",
    "\n",
    "\n",
    "**6. Deployment**\n",
    "\n",
    "This week.\n",
    "Describe how the model might be deployed and used at the business. Will there be an API that customer service reps can use when customers call? Should there be a system where a report gets sent to someone in customer retention or marketing with at-risk customers? We should really think about these things in the first step, although we can consider them here this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "churn_raw = pd.read_csv('../Week 2/churn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I forgot to fill the missing values of TotalCharges last week so here is my new class\n",
    "#     to clean the data. I'm filling them with the mode of TotalCharges, since it's so \n",
    "#     skewed I figured that would be the most likely value for it to be.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "# I hate having to rerun multiple cells though, so\n",
    "class InitAttributeCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "            return self # Nothing to do here, apparently\n",
    "    def transform(self, X, y=None):        \n",
    "        \n",
    "        ### customerID : DROP\n",
    "        X = X.drop(['customerID'], axis=1).copy()\n",
    "\n",
    "        \n",
    "        ### New column : average charge\n",
    "        # Fill TotalCharges with the mode of the dataset because it's so heavily skewed\n",
    "        X['TotalCharges'] = X['TotalCharges'].fillna(X['TotalCharges'].mode().iloc[0]).copy()\n",
    "        \n",
    "        X['average_charge'] = X['TotalCharges'] / X['tenure']\n",
    "        \n",
    "        # We've got some infs here so we're going to replace our nulls with our average average charge\n",
    "        X.loc[X['average_charge'] == np.inf, 'average_charge'] = 0#X['average_charge'].mean()\n",
    "        \n",
    "        # Now normalize\n",
    "        # Create new normalized column\n",
    "        X['average_charge_normal'] = ((X['average_charge'] - X['average_charge'].min()) /\n",
    "                            (X['average_charge'].max() - X['average_charge'].min()))\n",
    "\n",
    "        # Drop old column\n",
    "        X = X.drop(['average_charge'], axis=1)\n",
    "\n",
    "\n",
    "        ### tenure : bins\n",
    "        X['tenure_bins'] = pd.qcut(X['tenure'], q=10, labels=[i for i in range(0,10)]).cat.codes\n",
    "        X = X.drop(['tenure'], axis=1)\n",
    "        \n",
    "        \n",
    "        ### PhoneService : binarize\n",
    "        # Get our binary column\n",
    "        phoneservice_binary = pd.get_dummies(X['PhoneService'], drop_first=True, prefix='PhoneService')\n",
    "        \n",
    "        # Concatenate it with our dataframe\n",
    "        X = pd.concat([X, phoneservice_binary], axis=1)\n",
    "        \n",
    "        # and now drop the column that's been processed\n",
    "        X = X.drop(['PhoneService'], axis=1)\n",
    "\n",
    "        \n",
    "        ### Contract : onehotencode\n",
    "        # get our dummy columns\n",
    "        contract_dummies = pd.get_dummies(X['Contract'], drop_first=True, prefix='Contract')\n",
    "\n",
    "        # Concat them with our dataframe\n",
    "        X = pd.concat([X, contract_dummies], axis=1)\n",
    "\n",
    "        # and now drop the column\n",
    "        X = X.drop(['Contract'], axis=1)\n",
    "\n",
    "        \n",
    "        # PaymentMethod : onehotencode\n",
    "        # get our dummy columns\n",
    "        paymentmethod_dummies = pd.get_dummies(X['PaymentMethod'], drop_first=True, prefix='PaymentMethod', columns=[''])\n",
    "\n",
    "        # Concat them with our dataframe\n",
    "        X = pd.concat([X, paymentmethod_dummies], axis=1)\n",
    "\n",
    "        # and now drop the column\n",
    "        X = X.drop(['PaymentMethod'], axis=1)\n",
    "\n",
    "        \n",
    "        ### MonthlyCharges : normalize\n",
    "        # Create new normalized column\n",
    "        X['MonthlyCharges_normal'] = ((X['MonthlyCharges'] - X['MonthlyCharges'].min()) /\n",
    "                            (X['MonthlyCharges'].max() - X['MonthlyCharges'].min()))\n",
    "\n",
    "        # Drop old column\n",
    "        X = X.drop(['MonthlyCharges'], axis=1)\n",
    "\n",
    "            \n",
    "        ### TotalCharges : apply log\n",
    "        \n",
    "        \n",
    "        # apply log\n",
    "        totalcharges_log = X['TotalCharges'].apply(np.log)\n",
    "        \n",
    "        # change name before concat\n",
    "        totalcharges_log = totalcharges_log.rename('TotalCharges_log')\n",
    "        \n",
    "        #concat\n",
    "        X = pd.concat([X, totalcharges_log], axis=1)\n",
    "        \n",
    "        #drop old\n",
    "        X = X.drop(['TotalCharges'], axis=1)\n",
    "        \n",
    "        \n",
    "        ### Churn : binarize\n",
    "        # Get dummy\n",
    "        churn_binary = pd.get_dummies(X['Churn'], drop_first=True, prefix='Churn')\n",
    "\n",
    "        #combine\n",
    "        X = pd.concat([X, churn_binary], axis=1)\n",
    "\n",
    "        #drop\n",
    "        X = X.drop(['Churn'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = InitAttributeCleaner()\n",
    "churn = cleaner.fit_transform(churn_raw.copy()).copy()\n",
    "# churn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakup data into features and targets\n",
    "X = churn.drop(['Churn_Yes'], axis=1).copy()\n",
    "y = churn['Churn_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and targets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright well let's pick some models\n",
    "#     Random Forest is a classic so we'll try that one\n",
    "#     I know of the SGDClassifier - Stochastic Gradient Descent optimization of a linear\n",
    "#         classifier\n",
    "#     And why don't we use Naive Bayes since that's a classic \n",
    "#     And a decision tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "sgd_model = SGDClassifier(random_state=42)\n",
    "nb_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5634 entries, 4634 to 6041\n",
      "Data columns (total 10 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   average_charge_normal                  5634 non-null   float64\n",
      " 1   tenure_bins                            5634 non-null   int8   \n",
      " 2   PhoneService_Yes                       5634 non-null   uint8  \n",
      " 3   Contract_One year                      5634 non-null   uint8  \n",
      " 4   Contract_Two year                      5634 non-null   uint8  \n",
      " 5   PaymentMethod_Credit card (automatic)  5634 non-null   uint8  \n",
      " 6   PaymentMethod_Electronic check         5634 non-null   uint8  \n",
      " 7   PaymentMethod_Mailed check             5634 non-null   uint8  \n",
      " 8   MonthlyCharges_normal                  5634 non-null   float64\n",
      " 9   TotalCharges_log                       5634 non-null   float64\n",
      "dtypes: float64(3), int8(1), uint8(6)\n",
      "memory usage: 214.6 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to optimize our hyperparameters. So we're going to use a Randomized Search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint, truncnorm\n",
    "\n",
    "# We need to define some parameters to search for our different models\n",
    "rf_search_params = {'n_estimators' : randint(50, 800),\n",
    "                'max_features' : truncnorm(a=0.1, b=0.8, loc=0.5, scale=0.1),\n",
    "                'min_samples_split' : uniform(0.001, 0.1),\n",
    "                'max_features' : ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "sgd_search_params = {\n",
    "    'alpha' : uniform(0.0001, 0.05),\n",
    "    'max_iter' : randint(750, 2000),\n",
    "    'loss' : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    'penalty' : ['l2', 'l1', 'elasticnet'],\n",
    "    'learning_rate' : ['optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : uniform(0.001, 0.1)\n",
    "    \n",
    "}\n",
    "\n",
    "# Naive Bayes doesn't have any hyperparameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune our RandomForest\n",
    "rf_rscv = RandomizedSearchCV(rf_model, rf_search_params, n_iter=500, cv=5,\n",
    "                             n_jobs=-1, scoring='accuracy')\n",
    "rf_rscv.fit(X_train, y_train)\n",
    "\n",
    "rf_best = rf_rscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now the SGD\n",
    "sgd_rscv = RandomizedSearchCV(sgd_model, sgd_search_params, n_iter=500, cv=5,\n",
    "                              n_jobs=-1, scoring='accuracy')\n",
    "sgd_rscv.fit(X_train, y_train)\n",
    "\n",
    "sgd_best = sgd_rscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and also the NB\n",
    "nb_best = GaussianNB()\n",
    "nb_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest train score : 0.7983670571529996, test score: 0.7998580553584103\n",
      "SGD train score : 0.793574724884629, test score: 0.7970191625266146\n",
      "Naive Bayes train score : 0.7385516506922257, test score: 0.7444996451383961\n",
      "No Information train score : 0.7346467873624423, test score: 0.7345635202271115\n"
     ]
    }
   ],
   "source": [
    "### Alright let's evaluate the scores on the train and test set for each\n",
    "\n",
    "# RandomForest\n",
    "rf_train_score = rf_best.score(X_train, y_train)\n",
    "rf_test_score = rf_best.score(X_test, y_test)\n",
    "print(f'RandomForest train score : {rf_train_score}, test score: {rf_test_score}')\n",
    "\n",
    "# SGD\n",
    "sgd_train_score = sgd_best.score(X_train, y_train)\n",
    "sgd_test_score = sgd_best.score(X_test, y_test)\n",
    "print(f'SGD train score : {sgd_train_score}, test score: {sgd_test_score}')\n",
    "\n",
    "# NB\n",
    "nb_train_score = nb_best.score(X_train, y_train)\n",
    "nb_test_score = nb_best.score(X_test, y_test)\n",
    "print(f'Naive Bayes train score : {nb_train_score}, test score: {nb_test_score}')\n",
    "\n",
    "# No Information\n",
    "ni_train_score = y_train.value_counts()[0] / (y_train.value_counts()[0] + y_train.value_counts()[1])\n",
    "ni_test_score = y_test.value_counts()[0] / (y_test.value_counts()[0] + y_test.value_counts()[1])\n",
    "print(f'No Information train score : {ni_train_score}, test score: {ni_test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a small description of how we might use this ML algorithm in a business setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your summary here.\n",
    "\n",
    "-i had to fill nulls in totalcharges/average charges\n",
    "-i had to remove infinitys in average charges from diving by 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
